La inteligencia artificial explicable (en inglés: explainable artificial intelligence, habitualmente abreviado XAI) se refiere a métodos y técnicas en la aplicación de tecnología de inteligencia artificial (AI) por los que el ser humano es capaz de comprender las decisiones y predicciones realizadas por la inteligencia artificial. Contrasta con el concepto de la "caja negra" en aprendizaje automático (en inglés: machine learning), donde ni siquiera sus diseñadores pueden explicar por qué la IA ha realizado una decisión concreta.[1]​ XAI es una implementación del derecho social a la explicación.[2]​
El reto técnico de explicar las decisiones de IA se conoce como el problema de interpretabilidad.[3]​ Otra consideración es la sobrecarga informativa, así, la transparencia total no puede ser siempre posible o incluso requerida. Aun así, la simplificación a costo de engañar usuarios para aumentar la confianza o esconder los atributos indeseables del sistema tendrían que ser evitados permitiendo un equilibrio entre la interpretabilidad y la integridad de una explicación .[4]​ 
Los sistemas de IA optimizan el comportamiento para satisfacer un sistema de objetivos matemáticamente especificado elegido por los diseñadores del sistema, como el comando "maximizar la precisión en la evaluación de las críticas de películas en el conjunto de datos de prueba". La IA puede aprender reglas generales útiles del conjunto de pruebas, como "las revisiones que contienen la palabra 'horrible'" probablemente sean negativas ". Sin embargo, también puede aprender reglas inapropiadas, como "las revisiones que contienen 'Daniel Day-Lewis' suelen ser positivas"; tales reglas pueden ser indeseables si se considera que es probable que no se generalicen fuera del conjunto de pruebas, o si las personas consideran que la regla es "trampa" o "injusta".Un humano puede auditar las reglas en un XAI para tener una idea de la probabilidad de que el sistema se generalice a datos futuros del mundo real fuera del conjunto de pruebas.[3]​
La cooperación entre agentes, en este caso algoritmos y humanos, depende de la confianza. Si los humanos van a  aceptar prescripciones algorítmicas, necesitan confiar en ellas. La falta de completitud en la formalización de criterios de confianza es una barrera a aproximaciones de optimización sincera. Por esa razón, la interpretabilidad y la explicabilidad se postulan como objetivos intermedios para verificar otros criterios.[5]​
Los sistemas de inteligencia artificial a veces aprenden trucos indeseables que hacen un trabajo óptimo para satisfacer objetivos preprogramados explícitos en los datos de entrenamiento, pero que no reflejan los deseos implícitos complicados de los diseñadores de sistemas humanos. Por ejemplo, un sistema de 2017 encargado con el reconocimiento de imagen aprendió a "hacer trampa" al buscar una etiqueta de copyright que estaba asociada con imágenes de caballos, en lugar de aprender a saber si un caballo fue realmente fotografiado.[1]​ En otro sistema de 2017, una IA de aprendizaje supervisado encargada de captar los elementos en un mundo virtual aprendió a hacer trampa al colocar su manipulador entre el objeto y el espectador de tal manera que parecía falsamente estar captando el objeto.[6]​[7]​
Un proyecto de transparencia, el programa DARPA XAI, tiene como objetivo producir modelos de "caja de vidrio" que sean explicables a un "humano en el circuito", sin sacrificar el rendimiento IA. Los usuarios humanos deberían ser capaces de entender la cognición de la IA (tanto en tiempo real como después del hecho), y deberían poder determinar cuándo confiar en la IA y cuando se debe desconfiar de la IA.[8]​[9]​ Otras aplicaciones de XAI es extracción de conocimiento de modelos de cajas negras y comparaciones de modelos.[10]​ El término "caja de vidrio" también se ha usado para sistemas que monitorean las entradas y salidas de un sistema, con el propósito de verificar la adherencia del sistema a valores éticos y socio-legales y, por lo tanto, producir explicaciones basadas en valores. Además, el mismo término se ha utilizado para nombrar a un asistente de voz que produce declaraciones contrafactuales como explicaciones.[11]​
Durante las décadas de 1970 y 1990, sistemas de razonamiento simbólico, como MYCIN,[12]​ GUIDON, SOPHIE, y PROTOS fueron explorados para representar, razonar y explicar su razonamiento con fines de diagnóstico, instrucción o aprendizaje automático (aprendizaje basado en explicaciones).[13]​[14]​[15]​[16]​ MYCIN, desarrollado a principios de la década de 1970 como un prototipo de investigación para diagnosticar infecciones bacterianas del torrente sanguíneo, podría explicar cuál de sus reglas codificadas a mano contribuyó a un diagnóstico en un caso específico.[17]​ La investigación en sistemas inteligentes de tutoría desarrolló sistemas como SOPHIE que podrían actuar como un "experto articulado", explicando la estrategia de resolución de problemas a un nivel que el estudiante pudiera entender, para que supieran qué acción tomar a continuación. Por ejemplo, SOPHIE podría explicar el razonamiento cualitativo detrás de su solución de problemas electrónicos, aunque finalmente se basó en el simulador de circuito SPICE. Del mismo modo, GUIDON agregó reglas tutoriales para complementar las reglas de nivel de dominio de MYCIN para que pueda explicar la estrategia para el diagnóstico médico. Los enfoques simbólicos para el aprendizaje automático, especialmente aquellos que dependen del aprendizaje basado en explicaciones, como PROTOS, se basaron explícitamente en representaciones de explicaciones, tanto para explicar sus acciones como para adquirir nuevos conocimientos.
Desde la década de 1980 hasta principios de la década de 1990, se desarrollaron sistemas de mantenimiento de la verdad (TMS) para ampliar las capacidades de los sistemas de inferencia basados en el razonamiento causal, basados en reglas y en la lógica.[18]​: 360–362  Un TMS actúa para rastrear explícitamente líneas alternativas de razonamiento, justificaciones de conclusiones y líneas de razonamiento que conducen a contradicciones, permitiendo que el razonamiento futuro evite estos callejones sin salida. Para proporcionar una explicación, rastrean el razonamiento desde conclusiones hasta suposiciones a través de operaciones de reglas o inferencias lógicas, permitiendo que se generen explicaciones a partir de los rastros de razonamiento.  Como ejemplo, considere un solucionador de problemas basado en reglas con solo unas pocas reglas sobre Sócrates que concluye que ha muerto por veneno:
En la década de 1990, los investigadores también comenzaron a estudiar si es posible extraer de manera significativa las reglas no codificadas a mano generadas por redes neuronales opacas entrenadas.[20]​ Los investigadores en sistemas de expertos clínicos que crean apoyo de decisión basado en redes neuronales para los médicos han tratado de desarrollar explicaciones dinámicas que permitan que estas tecnologías sean más confiables en la práctica.[2]​ En el 2010, las preocupaciones públicas sobre el sesgo racial y de otro tipo en el uso de la IA para las decisiones de sentencias penales y los hallazgos de solvencia crediticia pueden haber llevado a una mayor demanda de inteligencia artificial transparente.[1]​ Como resultado, muchos académicos y organizaciones están desarrollando herramientas para ayudar a detectar sesgos en sus sistemas.[21]​
Marvin Minsky et al. plantearon la cuestión de que la IA puede funcionar como una forma de vigilancia, con los sesgos inherentes a la vigilancia, lo que apunta a la inteligencia humanista como una forma de crear una IA de "humano en el circuito" más justa y equilibrada.[22]​
Las modernas técnicas complejas de IA, como el aprendizaje profundo y los algoritmos genéticos, son naturalmente opacas.[23]​ Para abordar este problema, se han desarrollado muchos métodos nuevos para hacer que los nuevos modelos sean más explicables e interpretables.[24]​[25]​[26]​[27]​ Esto incluye muchos métodos, como la propagación de relevancia en capas (LRP), una técnica para determinar qué características en un vector de entrada particular contribuyen más fuertemente a la salida de una red neuronal.[28]​[29]​[30]​  Se han desarrollado otras técnicas para explicar una predicción particular realizada por un modelo de caja negra (no lineal), un objetivo denominado "interpretabilidad local".[31]​[32]​[33]​[34]​[35]​[36]​ Además, se ha trabajado en árboles de decisión y redes bayesianas, que son más transparentes para la inspección.[37]​ En 2018, se estableció una conferencia interdisciplinaria llamada FAT* (en inglés: Fairness, Accountability, and Transparency, «Equidad, responsabilidad y transparencia») para estudiar la transparencia y la explicabilidad en el contexto de los sistemas socio-técnicos, muchos de los cuales incluyen inteligencia artificial.[38]​[39]​
Como reguladores, los organismos oficiales y los usuarios en general dependen de los sistemas dinámicos basados en IA, se requerirá una responsabilidad más clara para los procesos de toma de decisiones para garantizar la confianza y la transparencia. La presentación de la primera conferencia mundial dedicada exclusivamente a esta disciplina emergente, la Conferencia Conjunta Internacional sobre Inteligencia Artificial: Taller sobre Inteligencia Artificial Explicable, es una muestra de que este requisito está ganando más impulso.[40]​
La Unión Europea introdujo un derecho de explicación en el Reglamento General de Protección de Datos (GDPR) como un intento de abordar los posibles problemas derivados de la creciente importancia de los algoritmos. La implementación del control empezó en 2018. Sin embargo, el derecho de explicación en GDPR cubre solo el aspecto local de la interpretabilidad. En los Estados Unidos, las compañías de seguros deben poder explicar sus decisiones sobre las tasas y cobertura.[41]​
XAI ha sido investigado en muchos sectores, incluyendo:
 [cs.CV]. 
 [cs.LG]. 
 [cs.CV]. 
